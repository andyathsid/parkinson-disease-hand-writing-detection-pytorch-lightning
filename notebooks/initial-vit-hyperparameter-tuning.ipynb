{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.seed_everything(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split into training and test sets successfully.\n"
     ]
    }
   ],
   "source": [
    "#Prepare dataset for training\n",
    "image_path = '../data/CombinedAll'\n",
    "dest_dir = '../data/CombinedAll'\n",
    "categories = ['parkinson', 'sehat']\n",
    "\n",
    "# Create destination directories\n",
    "for category in categories:\n",
    "    os.makedirs(os.path.join(dest_dir, 'train', category), exist_ok=True)\n",
    "    os.makedirs(os.path.join(dest_dir, 'test', category), exist_ok=True)\n",
    "\n",
    "# Split and copy files\n",
    "for category in categories:\n",
    "    category_path = os.path.join(image_path, category)\n",
    "    files = os.listdir(category_path)\n",
    "    train_files, test_files = train_test_split(files, test_size=0.2, random_state=42)\n",
    "    \n",
    "    for file in train_files:\n",
    "        shutil.copy(os.path.join(category_path, file), os.path.join(dest_dir, 'train', category, file))\n",
    "    \n",
    "    for file in test_files:\n",
    "        shutil.copy(os.path.join(category_path, file), os.path.join(dest_dir, 'test', category, file))\n",
    "\n",
    "print(\"Dataset split into training and test sets successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup path to data folder\n",
    "data_path = Path(\"../data/\")\n",
    "image_path = data_path / \"CombinedAll\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('../data/CombinedAll/train'),\n",
       " WindowsPath('../data/CombinedAll/test'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup Dirs\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.seed_everything(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DataModule(L.LightningDataModule):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         data_dir: str,\n",
    "#         batch_size: int = 32,\n",
    "#         num_workers: int = 8,\n",
    "#         image_size: int = 224\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.data_dir = data_dir\n",
    "#         self.batch_size = batch_size\n",
    "#         self.num_workers = num_workers\n",
    "#         self.image_size = image_size\n",
    "#         self.transform = transforms.Compose([\n",
    "#             transforms.Resize((self.image_size, self.image_size)),\n",
    "#             transforms.ToTensor()\n",
    "#         ])\n",
    "\n",
    "#         self.train_transform = transforms.Compose([\n",
    "#             transforms.Resize((image_size, image_size)),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "#             transforms.RandomRotation(10),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize(\n",
    "#                 mean=[0.485, 0.456, 0.406],\n",
    "#                 std=[0.229, 0.224, 0.225]\n",
    "#             )\n",
    "#         ])\n",
    "        \n",
    "#         self.val_transforms = transforms.Compose([\n",
    "#             transforms.Resize((image_size, image_size)),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize(\n",
    "#                 mean=[0.485, 0.456, 0.406],\n",
    "#                 std=[0.229, 0.224, 0.225]\n",
    "#             )\n",
    "#         ])\n",
    "        \n",
    "#     def setup(self, stage=None):\n",
    "#         if stage == 'fit' or stage is None:\n",
    "#             self.train_dataset = datasets.ImageFolder(\n",
    "#                 root=os.path.join(self.data_dir, 'train'),\n",
    "#                 transform=self.train_transforms\n",
    "#             )\n",
    "#             self.val_dataset = datasets.ImageFolder(\n",
    "#                 root=os.path.join(self.data_dir, 'test'),\n",
    "#                 transform=self.val_transforms\n",
    "#             )\n",
    "    \n",
    "#     def train_dataloader(self):\n",
    "#         return DataLoader(\n",
    "#             self.train_dataset,\n",
    "#             batch_size=self.batch_size,\n",
    "#             shuffle=True,\n",
    "#             num_workers=self.num_workers,\n",
    "#             pin_memory=True\n",
    "#             persistent_workers=True\n",
    "#         )\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         return DataLoader(\n",
    "#             self.val_dataset,\n",
    "#             batch_size=self.batch_size,\n",
    "#             shuffle=False,\n",
    "#             num_workers=self.num_workers,\n",
    "#             pin_memory=True\n",
    "#             persistent_workers=True\n",
    "#         )\n",
    "\n",
    "#     def test_dataloader(self):\n",
    "#         return self.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = os.cpu_count() - 8\n",
    "num_workers\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vitb16_0_weights = models.ViT_B_16_Weights.DEFAULT \n",
    "vitb16_0_transforms = vitb16_0_weights.transforms()\n",
    "print(vitb16_0_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "   [\n",
    "      transforms.Resize(256),\n",
    "      transforms.CenterCrop(224),\n",
    "      # transforms.RandomApply([transforms.RandomRotation(10)], p=0.5),\n",
    "      # transforms.RandomHorizontalFlip(p=0.5),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "   ],\n",
    ")\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "   [\n",
    "      transforms.Resize(256),\n",
    "      transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "   ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset ImageFolder\n",
       "     Number of datapoints: 1011\n",
       "     Root location: ..\\data\\CombinedAll\\train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
       "                CenterCrop(size=(224, 224))\n",
       "                ToTensor()\n",
       "                Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "            ),\n",
       " Dataset ImageFolder\n",
       "     Number of datapoints: 253\n",
       "     Root location: ..\\data\\CombinedAll\\test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
       "                CenterCrop(size=(224, 224))\n",
       "                ToTensor()\n",
       "                Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "            ))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder(test_dir, transform=test_transform)\n",
    "train_dataset,val_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L.seed_everything(123)\n",
    "# NUM_IMAGES = 4\n",
    "# images = [train_dataset[idx][0] for idx in range(NUM_IMAGES)]\n",
    "# orig_images = [Image.open(train_dataset.samples[idx][0]) for idx in range(NUM_IMAGES)]\n",
    "# orig_images = [test_transform(img) for img in orig_images]\n",
    "\n",
    "# img_grid = torchvision.utils.make_grid(torch.stack(images, dim=0), nrow=4, normalize=True, pad_value=0.5)\n",
    "# img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.title(\"Augmentation examples\")\n",
    "# plt.imshow(img_grid)\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as F\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# folder_path = \"../data/CombinedAll/sehat\"\n",
    "\n",
    "\n",
    "# image_files = [f for f in os.listdir(folder_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "# selected_images = random.sample(image_files, 4)\n",
    "\n",
    "\n",
    "# transform_list = [\n",
    "#     (\"Horizontal Flip\", transforms.RandomHorizontalFlip(p=1.0)),\n",
    "#     (\"Rotate 45Â°\", transforms.RandomRotation(degrees=(45, 45))),\n",
    "#     (\"Vertical Flip\", transforms.RandomVerticalFlip(p=1.0)),\n",
    "#     (\"Color Jitter\", transforms.ColorJitter(brightness=0.5))\n",
    "# ]\n",
    "\n",
    "\n",
    "# original_images = []\n",
    "# transformed_images = []\n",
    "# titles = []\n",
    "\n",
    "# for idx, img_file in enumerate(selected_images):\n",
    "#     img_path = os.path.join(folder_path, img_file)\n",
    "#     img = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "#     original_images.append(img)\n",
    "    \n",
    "#     transform = transform_list[idx][1]\n",
    "#     transformed_img = transform(img)\n",
    "    \n",
    "#     transformed_images.append(transformed_img)\n",
    "#     titles.append(transform_list[idx][0])\n",
    "\n",
    "# fig, axes = plt.subplots(2, 4, figsize=(30, 20))\n",
    "\n",
    "# for idx, img in enumerate(original_images):\n",
    "#     axes[idx//2, idx%2*2].imshow(img)\n",
    "#     axes[idx//2, idx%2*2].set_title('Original',fontsize=30, color='red')\n",
    "#     axes[idx//2, idx%2*2].axis('off')\n",
    "\n",
    "# for idx, (img, title) in enumerate(zip(transformed_images, titles)):\n",
    "#     axes[idx//2, idx%2*2 + 1].imshow(img)\n",
    "#     axes[idx//2, idx%2*2 + 1].set_title(f'Transformed: {title}', fontsize=30, color='green')\n",
    "#     axes[idx//2, idx%2*2 + 1].axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "from torchmetrics.classification import BinaryConfusionMatrix\n",
    "import io\n",
    "import numpy as np\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.seed_everything(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformerClassifier(L.LightningModule):\n",
    "    def __init__(self, trial=None):\n",
    "        super().__init__()\n",
    "        self.vit = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n",
    "        self.trial = trial\n",
    "        if self.trial:\n",
    "            freeze_backbone = trial.suggest_categorical(\"freeze_backbone\", [True, False])\n",
    "            if freeze_backbone:\n",
    "                for param in self.vit.parameters():\n",
    "                    param.requires_grad = False\n",
    "                \n",
    "        if self.trial:\n",
    "            dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5)\n",
    "            \n",
    "        self.vit.heads = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(in_features=768, out_features=1)\n",
    "        )\n",
    "            \n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        self.confusion_matrix = BinaryConfusionMatrix()\n",
    "    \n",
    "        self.val_preds = []\n",
    "        self.val_labels = []\n",
    "        self.training_step_outputs = []\n",
    "        self.training_epoch_losses = []\n",
    "        self.training_epoch_accs = []\n",
    "        self.validation_epoch_losses = []\n",
    "        self.validation_epoch_accs = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.vit(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.float()\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits.squeeze(), y)\n",
    "        preds = torch.sigmoid(logits.squeeze()) > 0.5\n",
    "        acc = (preds == y).float().mean()        \n",
    "        self.log(\n",
    "            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        \n",
    "        self.training_step_outputs.append({'loss': loss, 'acc': acc})\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.float()\n",
    "        logits = self(x)\n",
    "        preds = torch.sigmoid(logits.squeeze()) > 0.5\n",
    "        acc = (preds == y).float().mean()\n",
    "        loss = self.loss_fn(logits.squeeze(), y)\n",
    "        self.log(\n",
    "            \"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        \n",
    "        self.val_preds.append(preds)\n",
    "        self.val_labels.append(y)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.float()\n",
    "        logits = self(x)\n",
    "        preds = torch.sigmoid(logits.squeeze()) > 0.5\n",
    "        acc = (preds == y).float().mean()        \n",
    "        loss = self.loss_fn(logits.squeeze(), y)\n",
    "        self.log('test_loss', loss)\n",
    "        self.log(\"test_acc\", acc)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if self.trial:\n",
    "            optimizer_name = self.trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "            lr = self.trial.suggest_float(\"lr\", 1e-6, 1e-1, log=True)\n",
    "            optimizer = getattr(torch.optim, optimizer_name)(self.parameters(), lr=lr)\n",
    "            \n",
    "            return optimizer\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=1e-03)\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, T_max=10, eta_min=1e-6\n",
    "            )\n",
    "            return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"monitor\": \"val_loss\",\n",
    "                },            \n",
    "            }\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = self.trainer.callback_metrics['train_loss']\n",
    "        avg_acc = self.trainer.callback_metrics['train_acc']\n",
    "        \n",
    "        self.training_epoch_losses.append(avg_loss.item())\n",
    "        self.training_epoch_accs.append(avg_acc.item())\n",
    "            \n",
    "        \n",
    "        if len(self.training_epoch_losses) > 0:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            \n",
    "            # Loss plot\n",
    "            # Omit the first value from the list as its messes up the scale\n",
    "            ax1.plot(self.training_epoch_losses[1:], label='Train Loss')\n",
    "            if self.validation_epoch_losses:\n",
    "                ax1.plot(self.validation_epoch_losses[1:], label='Val Loss')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.legend()\n",
    "            \n",
    "            # Accuracy plot\n",
    "            ax2.plot(self.training_epoch_accs[1:], label='Train Accuracy')\n",
    "            if self.validation_epoch_accs:\n",
    "                ax2.plot(self.validation_epoch_accs[1:], label='Val Accuracy')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('Accuracy')\n",
    "            ax2.legend()\n",
    "            \n",
    "            # Log to tensorboard\n",
    "            buf = io.BytesIO()\n",
    "            plt.savefig(buf, format='png')\n",
    "            buf.seek(0)\n",
    "            im = transforms.ToTensor()(Image.open(buf))\n",
    "            self.logger.experiment.add_image('training_curves', im, global_step=self.current_epoch)\n",
    "            \n",
    "            plt.close()\n",
    "        \n",
    "        self.training_step_outputs.clear()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        all_preds = torch.cat(self.val_preds)\n",
    "        all_labels = torch.cat(self.val_labels)\n",
    "        self.confusion_matrix(all_preds, all_labels)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        self.confusion_matrix.plot(ax=ax, labels=[\"Healthy\", \"Patient\"])\n",
    "        \n",
    "        buf = io.BytesIO()\n",
    "        fig.savefig(buf, format=\"png\")\n",
    "        buf.seek(0)\n",
    "        im = transforms.ToTensor()(Image.open(buf))\n",
    "        \n",
    "        self.logger.experiment.add_image(\n",
    "            \"confusion_matrix\",\n",
    "            im,\n",
    "            global_step=self.current_epoch\n",
    "        )\n",
    "        \n",
    "        self.val_preds.clear()\n",
    "        self.val_labels.clear()\n",
    "        plt.close()\n",
    "\n",
    "        self.validation_epoch_losses.append(self.trainer.callback_metrics['val_loss'].item())\n",
    "        self.validation_epoch_accs.append(self.trainer.callback_metrics['val_acc'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers.tensorboard import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    model = VisionTransformerClassifier(trial)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\", verbose=False)\n",
    "    # pruner = PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")\n",
    "    logger = TensorBoardLogger(save_dir=\"../lightning_logs\", name=f\"vitb16_tuning_trial_{trial.number}\")\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "      max_epochs=20,\n",
    "      callbacks=[early_stopping],\n",
    "      logger=logger,\n",
    "      accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "      devices=\"auto\",\n",
    "      log_every_n_steps=1,\n",
    "      enable_progress_bar=False,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    val_loss = trainer.callback_metrics[\"val_loss\"].cpu().item()\n",
    "    val_acc = trainer.callback_metrics[\"val_acc\"].cpu().item()\n",
    "    return val_loss, val_acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 23:36:04,851] A new study created in memory with name: no-name-07afc5b0-5c28-4368-82b6-fe47f95dd7e3\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                  | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | vit              | VisionTransformer     | 85.8 M | train\n",
      "1 | loss_fn          | BCEWithLogitsLoss     | 0      | train\n",
      "2 | confusion_matrix | BinaryConfusionMatrix | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "85.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "85.8 M    Total params\n",
      "343.198   Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "[I 2024-12-24 23:41:23,834] Trial 0 finished with values: [0.3164321184158325, 0.9169960618019104] and parameters: {'freeze_backbone': False, 'dropout_rate': 0.4202761064089949, 'optimizer': 'Adam', 'lr': 8.043074578521919e-05}.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                  | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | vit              | VisionTransformer     | 85.8 M | train\n",
      "1 | loss_fn          | BCEWithLogitsLoss     | 0      | train\n",
      "2 | confusion_matrix | BinaryConfusionMatrix | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "85.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "85.8 M    Total params\n",
      "343.198   Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "[I 2024-12-24 23:48:46,722] Trial 1 finished with values: [0.14930643141269684, 0.9446640610694885] and parameters: {'freeze_backbone': False, 'dropout_rate': 0.09695059257094596, 'optimizer': 'SGD', 'lr': 0.0013561414893345516}.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                  | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | vit              | VisionTransformer     | 85.8 M | train\n",
      "1 | loss_fn          | BCEWithLogitsLoss     | 0      | train\n",
      "2 | confusion_matrix | BinaryConfusionMatrix | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "769       Trainable params\n",
      "85.8 M    Non-trainable params\n",
      "85.8 M    Total params\n",
      "343.198   Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "[I 2024-12-24 23:52:45,642] Trial 2 finished with values: [0.24622046947479248, 0.9051383137702942] and parameters: {'freeze_backbone': True, 'dropout_rate': 0.40446248661260786, 'optimizer': 'SGD', 'lr': 0.06993648699305559}.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                  | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | vit              | VisionTransformer     | 85.8 M | train\n",
      "1 | loss_fn          | BCEWithLogitsLoss     | 0      | train\n",
      "2 | confusion_matrix | BinaryConfusionMatrix | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "769       Trainable params\n",
      "85.8 M    Non-trainable params\n",
      "85.8 M    Total params\n",
      "343.198   Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "[I 2024-12-24 23:56:52,140] Trial 3 finished with values: [0.7215924859046936, 0.4940711557865143] and parameters: {'freeze_backbone': True, 'dropout_rate': 0.23013063091336633, 'optimizer': 'SGD', 'lr': 1.6659569344809358e-06}.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                  | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | vit              | VisionTransformer     | 85.8 M | train\n",
      "1 | loss_fn          | BCEWithLogitsLoss     | 0      | train\n",
      "2 | confusion_matrix | BinaryConfusionMatrix | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "769       Trainable params\n",
      "85.8 M    Non-trainable params\n",
      "85.8 M    Total params\n",
      "343.198   Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "[I 2024-12-25 00:01:31,741] Trial 4 finished with values: [0.5745351314544678, 0.7351778745651245] and parameters: {'freeze_backbone': True, 'dropout_rate': 0.3059800010610546, 'optimizer': 'SGD', 'lr': 3.172827347135285e-05}.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                  | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | vit              | VisionTransformer     | 85.8 M | train\n",
      "1 | loss_fn          | BCEWithLogitsLoss     | 0      | train\n",
      "2 | confusion_matrix | BinaryConfusionMatrix | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "769       Trainable params\n",
      "85.8 M    Non-trainable params\n",
      "85.8 M    Total params\n",
      "343.198   Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "[I 2024-12-25 00:05:43,464] Trial 5 finished with values: [0.7223515510559082, 0.40316206216812134] and parameters: {'freeze_backbone': True, 'dropout_rate': 0.21036948416033252, 'optimizer': 'Adam', 'lr': 4.611222579757379e-06}.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                  | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | vit              | VisionTransformer     | 85.8 M | train\n",
      "1 | loss_fn          | BCEWithLogitsLoss     | 0      | train\n",
      "2 | confusion_matrix | BinaryConfusionMatrix | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "85.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "85.8 M    Total params\n",
      "343.198   Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "[I 2024-12-25 00:13:04,022] Trial 6 finished with values: [0.6321669816970825, 0.6600790619850159] and parameters: {'freeze_backbone': False, 'dropout_rate': 0.03507608014982716, 'optimizer': 'SGD', 'lr': 1.0713391119057174e-05}.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                  | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | vit              | VisionTransformer     | 85.8 M | train\n",
      "1 | loss_fn          | BCEWithLogitsLoss     | 0      | train\n",
      "2 | confusion_matrix | BinaryConfusionMatrix | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "85.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "85.8 M    Total params\n",
      "343.198   Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "[I 2024-12-25 00:19:38,256] Trial 7 finished with values: [0.22415652871131897, 0.9407114386558533] and parameters: {'freeze_backbone': False, 'dropout_rate': 0.025851689575031755, 'optimizer': 'SGD', 'lr': 0.01902290859012587}.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                  | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | vit              | VisionTransformer     | 85.8 M | train\n",
      "1 | loss_fn          | BCEWithLogitsLoss     | 0      | train\n",
      "2 | confusion_matrix | BinaryConfusionMatrix | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "85.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "85.8 M    Total params\n",
      "343.198   Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "[I 2024-12-25 00:27:56,251] Trial 8 finished with values: [0.6391127705574036, 0.6640316247940063] and parameters: {'freeze_backbone': False, 'dropout_rate': 0.4775225171489029, 'optimizer': 'RMSprop', 'lr': 0.031317974427461044}.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                  | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | vit              | VisionTransformer     | 85.8 M | train\n",
      "1 | loss_fn          | BCEWithLogitsLoss     | 0      | train\n",
      "2 | confusion_matrix | BinaryConfusionMatrix | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "85.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "85.8 M    Total params\n",
      "343.198   Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "[I 2024-12-25 00:33:45,996] Trial 9 finished with values: [0.6539846658706665, 0.7588932514190674] and parameters: {'freeze_backbone': False, 'dropout_rate': 0.32093120813782716, 'optimizer': 'RMSprop', 'lr': 0.010728595896048382}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  10\n",
      "Best trials (Pareto front):\n",
      "  Trial Number: 1\n",
      "    Values (loss, accuracy): [0.14930643141269684, 0.9446640610694885]\n",
      "    Params: \n",
      "      freeze_backbone: False\n",
      "      dropout_rate: 0.09695059257094596\n",
      "      optimizer: SGD\n",
      "      lr: 0.0013561414893345516\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "    directions=[\"minimize\", \"maximize\"],\n",
    "    pruner=optuna.pruners.MedianPruner()\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=10)  \n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trials (Pareto front):\")\n",
    "for trial in study.best_trials:\n",
    "    print(f\"  Trial Number: {trial.number}\")\n",
    "    print(f\"    Values (loss, accuracy): {trial.values}\")\n",
    "    print(\"    Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"      {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna.visualization as vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis.plot_pareto_front(study, target_names=[\"val_loss\", \"val_acc\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".parkinson-disease-hand-writing-detection-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
